{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set your OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = #YOUR API KEY\n",
    "openai.api_key = #YOUR API KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will go into a link and scrape all <body> elements.\n",
    "This is a potential place for improvement, since we could be selectively just looking at the important bits.\n",
    "If we skip header, footer, nav menus etc, we would save GPT credits, time and money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_website(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')  # Use 'lxml' or 'html5lib' as the parser\n",
    "\n",
    "    # Find all \"body\" elements and extract their text\n",
    "    body_elements = soup.find_all('body')\n",
    "    text = \"\\n\".join([body.get_text() for body in body_elements])\n",
    "\n",
    "    # Remove repeated newline characters (more than two in a row)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the API call. I found that using gpt3.5turbo would give poor results.\n",
    "Promt engineering should definitely be improved before next run, but has been unchanged since last run.\n",
    "\"Type of Grant\" category should include \"disabled facilities\" as this is super common, contains boiler repairs and often confuses the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt4_response(url, LA_name):\n",
    "    result = extract_text_from_website(url)\n",
    "    prompt = \"The text you will look at is a parsed body of text from a website of a Local Council, which may or may not contain important information about grants available to homeowners. Your task is to understand available grants and log them into an important database. Based on this text parsed from the HTML file, give me a table with the following fields: Name of grant, Type of Grant (Whether it is Home Renewables, Home Energy Efficiency or Other), short grant description, URL of the grant, the amount of funding if available, list of measures that can be installed under this grant, start of funding date if available, end of funding date if available, conditions for eligibility and also any other notes. The columns will have the following names: [Name of Grant, Type of Grant, Grant Description, Grant URL, Amount of Funding, Measures, Start Date, End Date, Conditions for Eligibility, Other Notes]. If you struggle to fill any field of the table write NA. The values in Measure column must belong strictly to one of these categories: [Draft proof your external doors, Draft proof your windows, Install a biomass boiler (wood pellets), Install a gas combi boiler, Install a gas condensing boiler, Install a ground source heat pump, Install a log stove, Install a LPG combi boiler, Install a LPG condensing boiler, Install a new hot water tank, Install a solar hot water system, Install additional thermostatic controls, warm air systems, Install an air source heat pump, Install an oil combi boiler, Install an oil combi boiler (plus oil storage tank), Install an oil condensing boiler, Install an oil condensing boiler (plus oil storage tank), Install A-rated glazing (uPVC), Install cavity wall insulation, Install hot water tank insulation, Install hot water tank insulation and new controls, Install improved hot water controls, Install insulation for flat roofing, Install loft insulation, Install modern storage heaters, Install new insulated uPVC external doors, Install new radiators and distribution system, Install party wall insulation, Install room-in roof insulation, Install secondary glazing, Install solar PV panels, Install solid floor insulation, Install solid wall insulation, Install storage heater Celect type controls, Install suspended wooden floor insulation, Install thermostatic radiator valves, Install underfloor heating, Time and temperature zone control, Top-up your loft insulation, All heating measures, All insulation, All renewables, Non renewables]. The text to search is here: \"\n",
    "    system_msg = 'You are a helpful assistant mapping grant schemes available across the UK. You write grants from these schemes into an important database'\n",
    "    user_msg = prompt + result\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-4\",\n",
    "                                            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                                      {\"role\": \"user\", \"content\": user_msg}],\n",
    "                                            temperature=1)\n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function parses the API response and gets the table we require, saving it into a csv line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_append_to_csv(api_output, csv_filename, link, name):\n",
    "    # Check if the CSV file already exists, and create it with headers if it doesn't\n",
    "    try:\n",
    "        existing_data = pd.read_csv(csv_filename)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame(columns=[\"Name of Grant\", \"Grant Type\", \"Grant Description\", \"Grant URL\", \"Amount of Funding\", \"Measures\", \"Start Date\", \"End Date\", \"Conditions for Eligibility\", \"Other Notes\"])\n",
    "        existing_data.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Extract the content\n",
    "    content = api_output[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Split the content into rows\n",
    "    rows = content.split('\\n')\n",
    "\n",
    "    # Initialize a list to hold the data\n",
    "    table_data = []\n",
    "    header = None\n",
    "\n",
    "    for row in rows:\n",
    "        row = row.strip()\n",
    "        if row.startswith('|'):\n",
    "            # Extract columns using the pipe symbol as the delimiter\n",
    "            columns = [column.strip() for column in re.split(r'\\s*\\|\\s*', row) if column.strip()]\n",
    "            if not header:\n",
    "                header = columns\n",
    "            else:\n",
    "                if len(columns) == len(header):\n",
    "                    table_data.append(columns)\n",
    "\n",
    "    # Create a Pandas DataFrame with column names\n",
    "    df = pd.DataFrame(table_data, columns=header)\n",
    "    df['Link'] = link\n",
    "    df['Local Authority'] = name\n",
    "    df['Date Updated'] = pd.Timestamp(\"today\").strftime(\"%d/%m/%Y\")\n",
    "    filtered_df = df[~df.applymap(lambda x: isinstance(x, str) and '---' in x).any(axis=1)]\n",
    "\n",
    "    # Append the data to the existing CSV file\n",
    "    filtered_df.to_csv(csv_filename, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv(\"search_results.csv\")\n",
    "test_urls = websites[\"URL\"]\n",
    "test_councils = websites[\"Council Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438\n"
     ]
    }
   ],
   "source": [
    "print(len(test_councils))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code takes quite a while to run so likely, you will want to split things up into multiple files and then merge them manually at the end.\n",
    "Some errors still happen, mostly when the google search leads to a PDF. These are all noted in the error file and could be explored manually separately if time permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "Error processing url https://www.tmbc.gov.uk/downloads/file/3011/housing-assistance-policy: Rate limit reached for gpt-4 in organization org-yIC1FzVuNadajuXe1tIE6di4 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "Error re-processing urlhttps://www.tmbc.gov.uk/downloads/file/3011/housing-assistance-policy: Rate limit reached for gpt-4 in organization org-yIC1FzVuNadajuXe1tIE6di4 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "Error processing url https://www.warrington.gov.uk/sites/default/files/2019-08/low_cost_loans_info_sheet_august_13.pdf: Rate limit reached for gpt-4 in organization org-yIC1FzVuNadajuXe1tIE6di4 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "Error re-processing urlhttps://www.warrington.gov.uk/sites/default/files/2019-08/low_cost_loans_info_sheet_august_13.pdf: Rate limit reached for gpt-4 in organization org-yIC1FzVuNadajuXe1tIE6di4 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "Error processing url https://cms.wiltshire.gov.uk/documents/s20336/#:~:text=Home%20loans%20are%20subsidised%20loans,on%20behalf%20of%20Wiltshire%20Council.: Rate limit reached for gpt-4 in organization org-yIC1FzVuNadajuXe1tIE6di4 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "Error re-processing urlhttps://cms.wiltshire.gov.uk/documents/s20336/#:~:text=Home%20loans%20are%20subsidised%20loans,on%20behalf%20of%20Wiltshire%20Council.: Rate limit reached for gpt-4 in organization org-yIC1FzVuNadajuXe1tIE6di4 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "Error processing url https://www.wychavon.gov.uk/housing: You exceeded your current quota, please check your plan and billing details.\n",
      "Error re-processing urlhttps://www.wychavon.gov.uk/housing: You exceeded your current quota, please check your plan and billing details.\n",
      "1435\n",
      "Error processing url https://www.wyre.gov.uk/housing-options-homelessness/housing-options/2: You exceeded your current quota, please check your plan and billing details.\n",
      "Error re-processing urlhttps://www.wyre.gov.uk/housing-options-homelessness/housing-options/2: You exceeded your current quota, please check your plan and billing details.\n",
      "1436\n",
      "Error processing url https://www.wyreforestdc.gov.uk/housing-and-homes/owning-or-privately-renting/building-your-home/: You exceeded your current quota, please check your plan and billing details.\n",
      "Error re-processing urlhttps://www.wyreforestdc.gov.uk/housing-and-homes/owning-or-privately-renting/building-your-home/: You exceeded your current quota, please check your plan and billing details.\n",
      "1437\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "csv_filename = \"FullRun_011123_2.csv\"\n",
    "issues = {'council':[],'error':[],'url':[]}\n",
    "for i in range(1400, 1438):\n",
    "    try:\n",
    "        response = get_gpt4_response(test_urls[i], test_councils[i])\n",
    "        extract_and_append_to_csv(response, csv_filename, test_urls[i], test_councils[i])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing url {test_urls[i]}: {e}\")\n",
    "        try:\n",
    "            time.sleep(60)\n",
    "            response = get_gpt4_response(test_urls[i], test_councils[i])\n",
    "            extract_and_append_to_csv(response, csv_filename, test_urls[i], test_councils[i])\n",
    "        except Exception as e:\n",
    "            print(f\"Error re-processing url{test_urls[i]}: {e}\")\n",
    "            issues['council'].append(test_councils[i])\n",
    "            issues['error'].append(f\"Error re-processing url{test_urls[i]}: {e}\")\n",
    "            issues['url'].append(test_urls[i])\n",
    "            extract_and_append_to_csv(response, csv_filename, test_urls[i], test_councils[i])\n",
    "\n",
    "    print(i)\n",
    "\n",
    "issues_df = pd.DataFrame(issues)\n",
    "issues_df.to_csv(\"error_log_011123_2.csv\", mode='a', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
